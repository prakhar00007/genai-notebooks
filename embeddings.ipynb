{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPqT5McRIFHwYTOZhQA9hlY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prakhar00007/genai-notebooks/blob/main/embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## One Hot Encoding"
      ],
      "metadata": {
        "id": "lITUbFUPz6ZO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus=['I love NLP','I teach Gen AI',\"I am working on Euron\"]"
      ],
      "metadata": {
        "id": "1tkM7i_c0AZe"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus= [text.lower() for text in corpus]\n"
      ],
      "metadata": {
        "id": "bniFEiUW0RlQ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j6-Ne6DU0fkj",
        "outputId": "212279ce-b230-4b70-f419-49814eb5f790"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i love nlp', 'i teach gen ai', 'i am working on euron']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "unique_words=list(set(\" \".join(corpus).split()))"
      ],
      "metadata": {
        "id": "owuNMSTj0g7x"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_words\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44Nwtu5c09St",
        "outputId": "a118e1ab-5e8c-4484-aff7-a9960e5cfa14"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['love', 'working', 'ai', 'nlp', 'gen', 'euron', 'am', 'i', 'on', 'teach']"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "enumerate(unique_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "krRMGm001xgt",
        "outputId": "6fe39ebb-f596-4fe1-fc2f-b9649f83e95a"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<enumerate at 0x7d403a4ad350>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "enu=iter(enumerate(unique_words))"
      ],
      "metadata": {
        "id": "_jXOIlJu2ZUN"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "next(enu)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ombQ43g02oRV",
        "outputId": "be6b4db8-951f-4673-b2a0-528de6967106"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2, 'ai')"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in enumerate(unique_words):\n",
        "  print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XBwgpwLk2pYD",
        "outputId": "3f8b82fb-8d1c-4653-944c-9113741f0a8a"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(0, 'love')\n",
            "(1, 'working')\n",
            "(2, 'ai')\n",
            "(3, 'nlp')\n",
            "(4, 'gen')\n",
            "(5, 'euron')\n",
            "(6, 'am')\n",
            "(7, 'i')\n",
            "(8, 'on')\n",
            "(9, 'teach')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i, word in enumerate(unique_words):\n",
        "    print(f\"{word}: {i}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AMYwZwDR3FoB",
        "outputId": "8b7ebd22-a553-409f-9270-f5841b00ccfd"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "love: 0\n",
            "working: 1\n",
            "ai: 2\n",
            "nlp: 3\n",
            "gen: 4\n",
            "euron: 5\n",
            "am: 6\n",
            "i: 7\n",
            "on: 8\n",
            "teach: 9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_to_index = {word: i for i, word in enumerate(unique_words)}\n"
      ],
      "metadata": {
        "id": "WBGbHe5K3kOr"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_to_index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jvdoVj-l4S3u",
        "outputId": "f6f14f27-c2c3-497c-a0ae-a628fbb0d996"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'love': 0,\n",
              " 'working': 1,\n",
              " 'ai': 2,\n",
              " 'nlp': 3,\n",
              " 'gen': 4,\n",
              " 'euron': 5,\n",
              " 'am': 6,\n",
              " 'i': 7,\n",
              " 'on': 8,\n",
              " 'teach': 9}"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ece0N0mz4UKi",
        "outputId": "4fb98651-856c-4460-c20c-9a0e9a598f0d"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i love nlp', 'i teach gen ai', 'i am working on euron']"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "one_hot_encoder=[]\n",
        "for sentence in corpus:\n",
        "  print(sentence)\n",
        "  sentence_vector=[]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YcdM8TNi47Kr",
        "outputId": "29280f63-4889-4f93-bc0a-5f1c8b5937e4"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i love nlp\n",
            "i teach gen ai\n",
            "i am working on euron\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "one_hot_vecotor = []\n",
        "for sentence in corpus:\n",
        "    print(sentence)\n",
        "    sentence_vector = []\n",
        "    for word in sentence.split():\n",
        "        vector = [0] * len(unique_words)\n",
        "        print(vector)\n",
        "        vector[word_to_index[word]] = 1\n",
        "        print('word to index',word_to_index[word])\n",
        "        print(vector[word_to_index[word]])\n",
        "        print('sentece vector befor appending',sentence_vector)\n",
        "        sentence_vector.append(vector)\n",
        "        print('sentence vector after appending',sentence_vector)\n",
        "\n",
        "    one_hot_vecotor.append(sentence_vector)\n",
        "    print('-----------')\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cHElCbbE4-IC",
        "outputId": "e0129855-fe24-4f5d-8a8d-acdb2f8c49c3"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i love nlp\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "word to index 7\n",
            "1\n",
            "sentece vector befor appending []\n",
            "sentence vector after appending [[0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "word to index 0\n",
            "1\n",
            "sentece vector befor appending [[0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]\n",
            "sentence vector after appending [[0, 0, 0, 0, 0, 0, 0, 1, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "word to index 3\n",
            "1\n",
            "sentece vector befor appending [[0, 0, 0, 0, 0, 0, 0, 1, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
            "sentence vector after appending [[0, 0, 0, 0, 0, 0, 0, 1, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]]\n",
            "-----------\n",
            "i teach gen ai\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "word to index 7\n",
            "1\n",
            "sentece vector befor appending []\n",
            "sentence vector after appending [[0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "word to index 9\n",
            "1\n",
            "sentece vector befor appending [[0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]\n",
            "sentence vector after appending [[0, 0, 0, 0, 0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "word to index 4\n",
            "1\n",
            "sentece vector befor appending [[0, 0, 0, 0, 0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]]\n",
            "sentence vector after appending [[0, 0, 0, 0, 0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 1, 0, 0, 0, 0, 0]]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "word to index 2\n",
            "1\n",
            "sentece vector befor appending [[0, 0, 0, 0, 0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 1, 0, 0, 0, 0, 0]]\n",
            "sentence vector after appending [[0, 0, 0, 0, 0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]]\n",
            "-----------\n",
            "i am working on euron\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "word to index 7\n",
            "1\n",
            "sentece vector befor appending []\n",
            "sentence vector after appending [[0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "word to index 6\n",
            "1\n",
            "sentece vector befor appending [[0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]\n",
            "sentence vector after appending [[0, 0, 0, 0, 0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 0, 0, 1, 0, 0, 0]]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "word to index 1\n",
            "1\n",
            "sentece vector befor appending [[0, 0, 0, 0, 0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 0, 0, 1, 0, 0, 0]]\n",
            "sentence vector after appending [[0, 0, 0, 0, 0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 0, 0, 1, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "word to index 8\n",
            "1\n",
            "sentece vector befor appending [[0, 0, 0, 0, 0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 0, 0, 1, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
            "sentence vector after appending [[0, 0, 0, 0, 0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 0, 0, 1, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 1, 0]]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "word to index 5\n",
            "1\n",
            "sentece vector befor appending [[0, 0, 0, 0, 0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 0, 0, 1, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 1, 0]]\n",
            "sentence vector after appending [[0, 0, 0, 0, 0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 0, 0, 1, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]\n",
            "-----------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "one_hot_vecotor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33vlI2-k7KyY",
        "outputId": "de7757f3-4550-42c7-fd65-b5ae38a8fe20"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[[0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
              "  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "  [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]],\n",
              " [[0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
              "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
              "  [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
              "  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]],\n",
              " [[0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
              "  [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
              "  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "  [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
              "  [0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]]"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Bag of Words (BOW)"
      ],
      "metadata": {
        "id": "LhiUEFdfMA2W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " corpus=['i love nlp','i teach gen ai','i am working with euron']"
      ],
      "metadata": {
        "id": "NWM2ezDW7wU7"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "metadata": {
        "id": "zwFaPMZlNR5t"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer=CountVectorizer()"
      ],
      "metadata": {
        "id": "RIqbEB69NeUX"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X=vectorizer.fit_transform(corpus)"
      ],
      "metadata": {
        "id": "Lkm_WFABNgzO"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.toarray()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I0_MVAjONici",
        "outputId": "7f0ce51c-d448-4b87-aaf2-5d18ef9545f5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, 0, 1, 1, 0, 0, 0],\n",
              "       [1, 0, 0, 1, 0, 0, 1, 0, 0],\n",
              "       [0, 1, 1, 0, 0, 0, 0, 1, 1]])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TF-IDF"
      ],
      "metadata": {
        "id": "B_RpmGUgmaIU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus=['i love nlp','i teach gen ai','i am working with euron']"
      ],
      "metadata": {
        "id": "b5FiuFXGNzXD"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "metadata": {
        "id": "KDx-QUFLmguJ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vector_tf_idf= TfidfVectorizer()"
      ],
      "metadata": {
        "id": "AWcsEan1mr_e"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X= vector_tf_idf.fit_transform(corpus)"
      ],
      "metadata": {
        "id": "gAEB1YEVmx9k"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.toarray()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xhgtmO2hm6M5",
        "outputId": "bbbb3269-ca7c-4705-b594-7e85e416fb44"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.        , 0.        , 0.        , 0.        , 0.70710678,\n",
              "        0.70710678, 0.        , 0.        , 0.        ],\n",
              "       [0.57735027, 0.        , 0.        , 0.57735027, 0.        ,\n",
              "        0.        , 0.57735027, 0.        , 0.        ],\n",
              "       [0.        , 0.5       , 0.5       , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.5       , 0.5       ]])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H3lnTXM-nDsS",
        "outputId": "09de5f00-3909-44b0-b3a1-7db82e2730cd"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i love nlp', 'i teach gen ai', 'i am working with euron']"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(list(set(\" \".join(corpus).split())))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MEQQ9F1MoQaD",
        "outputId": "15719cf0-43e4-4718-ae87-f63336b5d7c2"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_new=['pappu nach nahi sakta','pappu loves dancing', 'pappu stops dancing']"
      ],
      "metadata": {
        "id": "LQKVQmH8odyo"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "metadata": {
        "id": "IFY4pdV-xprf"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vector_tf_idf= TfidfVectorizer()"
      ],
      "metadata": {
        "id": "8lH0VbS9xvuw"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X=vector_tf_idf.fit_transform(corpus_new)\n"
      ],
      "metadata": {
        "id": "Ld77ovtmx2Ie"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.toarray()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B2rQ0di7x7CH",
        "outputId": "327e1b6d-6127-4360-b11a-7baf55856c75"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.        , 0.        , 0.54645401, 0.54645401, 0.32274454,\n",
              "        0.54645401, 0.        ],\n",
              "       [0.54783215, 0.72033345, 0.        , 0.        , 0.42544054,\n",
              "        0.        , 0.        ],\n",
              "       [0.54783215, 0.        , 0.        , 0.        , 0.42544054,\n",
              "        0.        , 0.72033345]])"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "metadata": {
        "id": "X6oYes6ex9bi"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer= CountVectorizer()"
      ],
      "metadata": {
        "id": "NBmOgVVjyXhG"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x= vectorizer.fit_transform(corpus_new)"
      ],
      "metadata": {
        "id": "vcqUyDvqyZpP"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.toarray()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zkj_wJzsyfE8",
        "outputId": "b8e458cf-48d6-4cd5-9138-d5f3fb8a0782"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 1, 1, 1, 1, 0],\n",
              "       [1, 1, 0, 0, 1, 0, 0],\n",
              "       [1, 0, 0, 0, 1, 0, 1]])"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word2Vec"
      ],
      "metadata": {
        "id": "Bv47luac_nV6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "URbtt2rDAPMg",
        "outputId": "f7fbfbf2-808f-471c-ed5b-43e637cc73c9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.12/dist-packages (4.4.0)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.2)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.4.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import string\n",
        "import matplotlib.pyplot as plt\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import numpy as np\n",
        "import re"
      ],
      "metadata": {
        "id": "SqjZwwwiywVE"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22l23aFvygGE",
        "outputId": "b6222be9-d063-4095-edb6-56831b6a1b35"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus=[\n",
        "    'my name is Sudhanshu kumar',\n",
        "    'i use to Teach all data stack along with ops and cloud!',\n",
        "    'fadll dlkjfkl #@*234 lkjdf #34533',\n",
        "    'nlp is very Amazing',\n",
        "    'We are trying to learn word2vec',\n",
        "    'we will try to build two models for word2vec - cbow and skipgram',\n",
        "    'we will also working on cleaning the data which is going to have some noisy data. it\"s basically part of pre-processing',\n",
        "    'nlp is a part of AI',\n",
        "    'word2vec is better than one hot encoding, tf-idf,bags of words'\n",
        "\n",
        "]"
      ],
      "metadata": {
        "id": "ZTbcyAFSBp9V"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_corpus = [re.sub(r'\\d+', '', sentence.lower()) for sentence in corpus]\n"
      ],
      "metadata": {
        "id": "X7mda_keCb7y"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_corpus"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jh6tg3ixEuTf",
        "outputId": "23c63a03-8cfd-48b1-880a-03f83626a919"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['my name is sudhanshu kumar',\n",
              " 'i use to teach all data stack along with ops and cloud!',\n",
              " 'fadll dlkjfkl #@* lkjdf #',\n",
              " 'nlp is very amazing',\n",
              " 'we are trying to learn wordvec',\n",
              " 'we will try to build two models for wordvec - cbow and skipgram',\n",
              " 'we will also working on cleaning the data which is going to have some noisy data. it\"s basically part of pre-processing',\n",
              " 'nlp is a part of ai',\n",
              " 'wordvec is better than one hot encoding, tf-idf,bags of words']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_corpus = [''.join(ch for ch in sentence if ch not in string.punctuation) for sentence in cleaned_corpus]\n"
      ],
      "metadata": {
        "id": "XUF0pzC3F3Am"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_corpus\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YndzKsjHGdIF",
        "outputId": "c25c24a5-13e7-40e9-a77e-ae522bd26090"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['my name is sudhanshu kumar',\n",
              " 'i use to teach all data stack along with ops and cloud',\n",
              " 'fadll dlkjfkl  lkjdf ',\n",
              " 'nlp is very amazing',\n",
              " 'we are trying to learn wordvec',\n",
              " 'we will try to build two models for wordvec  cbow and skipgram',\n",
              " 'we will also working on cleaning the data which is going to have some noisy data its basically part of preprocessing',\n",
              " 'nlp is a part of ai',\n",
              " 'wordvec is better than one hot encoding tfidfbags of words']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_corpus = [\n",
        "    ' '.join([word for word in sentence.split() if word not in stopwords.words('english')])\n",
        "    for sentence in cleaned_corpus\n",
        "]\n",
        "\n",
        "print(filtered_corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OSikZq6SGfAa",
        "outputId": "199d9b50-1126-4d99-cbe4-44f1e707069b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['name sudhanshu kumar', 'use teach data stack along ops cloud', 'fadll dlkjfkl lkjdf', 'nlp amazing', 'trying learn wordvec', 'try build two models wordvec cbow skipgram', 'also working cleaning data going noisy data basically part preprocessing', 'nlp part ai', 'wordvec better one hot encoding tfidfbags words']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_corpus"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uGi9sJFHGo0L",
        "outputId": "9568a6ac-dffc-4e39-8ee9-2d9ecc58ba84"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['name sudhanshu kumar',\n",
              " 'use teach data stack along ops cloud',\n",
              " 'fadll dlkjfkl lkjdf',\n",
              " 'nlp amazing',\n",
              " 'trying learn wordvec',\n",
              " 'try build two models wordvec cbow skipgram',\n",
              " 'also working cleaning data going noisy data basically part preprocessing',\n",
              " 'nlp part ai',\n",
              " 'wordvec better one hot encoding tfidfbags words']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_corpus = [sentence.split() for sentence in filtered_corpus]\n",
        "tokenized_corpus\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UvIB95oKJpzX",
        "outputId": "24d22db1-8961-4cc4-8673-c970b0d17e6b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['name', 'sudhanshu', 'kumar'],\n",
              " ['use', 'teach', 'data', 'stack', 'along', 'ops', 'cloud'],\n",
              " ['fadll', 'dlkjfkl', 'lkjdf'],\n",
              " ['nlp', 'amazing'],\n",
              " ['trying', 'learn', 'wordvec'],\n",
              " ['try', 'build', 'two', 'models', 'wordvec', 'cbow', 'skipgram'],\n",
              " ['also',\n",
              "  'working',\n",
              "  'cleaning',\n",
              "  'data',\n",
              "  'going',\n",
              "  'noisy',\n",
              "  'data',\n",
              "  'basically',\n",
              "  'part',\n",
              "  'preprocessing'],\n",
              " ['nlp', 'part', 'ai'],\n",
              " ['wordvec', 'better', 'one', 'hot', 'encoding', 'tfidfbags', 'words']]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ZL_pD-11RgWp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word2vec_cbow=Word2Vec(sentences=tokenized_corpus,vector_size=100,window=5,min_count=1,sg=0)"
      ],
      "metadata": {
        "id": "886_it6_J94b"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word2vec_skipgram=Word2Vec(sentences=tokenized_corpus,vector_size=100,window=5,min_count=1,sg=1)"
      ],
      "metadata": {
        "id": "VmLmsC5UKoGX"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5HQpy2KfRsaV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}